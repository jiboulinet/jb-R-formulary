
library(Pmetrics)
#1) Define your error model using makeErrorPoly
makeErrorPoly(obs, sd)
#2)Create the PMtree in your working directory
setwd("C:/Documents and Settings/u850/Bureau/ratna_mmf")
PMtree("MMF")
#3)Put yours files in the Runs file and check your model
#check the files in your working directory using 
list.files()
PMcheck("mmf.csv", "model.txt", fix = F, quiet = F)
#4) run the model after checking your working directory
setwd("C:/Documents and Settings/u850/Bureau/ratna_mmf/Runs")
list.files()
NPrun(model="gammaem.txt",data="mmf.csv",cycles=500)

#5)load the results
PMload(1)

#6)GOF 
##Sphaghetti_plot
pdf(file="spaghetti plot.pdf")
plot(mdata.1,overlay=T,xlim=c(0,12))
dev.off()
##ipred dv
pdf(file="post.pdf")
  plot(op)
dev.off()
##residus
pdf(file="residus.pdf")
  plot(op.1,pred.type="post",resid=T, icen="median")#or pop
  par(ask=F)
dev.off()
ipred<-subset(op.1,pred.type=="post")#dv ip
pred<-subset(op.1,pred.type=="pop")#d
write.table(ipred,file="ipred_dv_time.txt")

##or function gof
replace x by the number of your run

gof<-function(x,...){
  op <- get(paste("op",x,sep="."))
  mdata <- get(paste("mdata",x,sep="."))
  par(ask=T)
  #spaghetti plot
pdf(file="spaghetti plot.pdf")
  plot(mdata,overlay=T,xlim=c(0,12))
dev.off()
  #ipred dv
pdf(file="pop.pdf")
  plot(op,pred.type="pop")
dev.off()
pdf(file="post.pdf")
  plot(op)
dev.off()
pdf(file="residus.pdf")
  plot(op,pred.type="post",resid=T, icen="median")#or pop
  par(ask=F)
dev.off()
ipred<-subset(op,pred.type=="post")#dv ip
pred<-subset(op,pred.type=="pop")#d
write.table(ipred,file="ipred_dv_time.txt")
}

gof(3)


#7)make individual plots
pdf(file="plot_eb.pdf")
plot(mdata.3,pred=post.3,overlay=F,pch=3,cex=1,lwd=2,join=F,doses=F,legend=F,log=F)

dev.off()


#8) try to change models or the limits of your parameters to increase the fit and rerun it and compare yours models

PMcompare(1,2,3)#you can add plots using plot=T


#9) first test association cov/param
#continuous cov
PMstep(cov.1)
#binary cov
wilcox.test(C0 ~sex, data=cov.1) 
#multicategorical cov
kruskal.test(CYP~C0,data=cov.1)

#10) include significants cov in your model file
#classical coding is param*(cov/median cov)**0.75 
#ex linear association CL=THETA(1)+THETA(2)*WT/70,exponential association: CL=THETA(1)*EXP(THETA(2)*WT/70) or power model CL=THETA(1)*(WT/70)**THETA(2) 
##binary cov CL=THETA(1) *THETA(2)**CYP3A5

#11) Internal validation put your model.txt and your data set in working dir sim
# and be sure to have made PMload for your "good" model and keep the number in mind (here is nÂ°4)
SIMrun(poppar=final.4, data = "mmf.csv", split = T, nsim = 1000, model = "gammaem.txt", include=1:5)
#parse the results
simdata <- SIMparse("simout*.txt", combine=T)
#VPC
?plot.PMsim
plot(simdata, mult = 1, log = T,probs = c(0.05, 0.5, 0.95), obs=op.4,ci=0,xlab="Time (h)",ylab="MPA concentration (mg/l)",ocol="darkgrey")

#12)Data_splitting
setwd("../Runs")
mdata<-PMreadMatrix("ex.csv")
mdata_1<-subset(mdata, !duplicated(id))
library(caret)
set.seed(107)
inTrain <- createDataPartition(y = mdata_1$id,  ## the outcome data are needed
                               p = .75,  ## The percentage of data in the ## training set
                               list = FALSE)## The format of the results
## The output is a set of integers for the rows of Sonar
## that belong in the training set.
training <- mdata[ mdata$id%in%inTrain,]
testing  <- mdata[!mdata$id%in%inTrain,]
PMwriteMatrix(training,"../src/exdev.csv")
PMwriteMatrix(testing,"../src/exval.csv")

#13) Run and load the model dev in the dev dataset
NPrun(model="model.txt", data="exdev.csv", run=17, overwrite=T)
PMload(17)

#14) create a dataset with the time of interest for the best LSS 
mmoptt= training[!is.na(training$out)&training$id==1,]
mmoptt$time<-round(test$time,0)
mmoptt2<-rbind(mdata[1,],mmoptt)
PMwriteMatrix(mmoptt2,"../src/mmoptt.csv")

#15) Optimal sampling determination for LSS
tps_opt<-MMopt(poppar=final.17, model ="model.txt", predInt=0,data = "mmoptt.csv", nsamp = 2, outeq = 1)
tps_opt
plot(tps_opt, log=F)

#16)extract points selected by MMopt at step 12 in the validation dataset after loading it
#function to select the optimal times

opt_t3<-function(a, b, c){
  testing1<-subset(testing, evid==0)
  init<-testing1[1,3]
  lss<-subset(testing,
              evid==1|
                (time>=(a-(0.2*(a-init)))&time<=(a+(0.2*(a-init))))
              |(time>(b-(0.2*(b-init)))&time<(b+(0.2*(b-init))))
              |(time>(c-(0.2*(c-init)))&time<(c+(0.2*(c-init))))
  )
  
  lss1<-lss[ order(lss$id,lss$time) ,]
  head(lss1)
  PMwriteMatrix(lss1,"lss.csv",override=T)
}
opt_t3(120, 121, 126)

#17)bayesian estimation and inference for new patient
NPrun(model=15,data="lss.csv", prior=NPdata.15, cycles=0)
PMload(16)

#18)plot curve straight line is the bayesian fitted curve with LSS and points are all the observed points
#loading validtino dataset with full points
exval<-PMreadMatrix("exval.csv")##here is 
plot(exval,pred=post.16,overlay=F,pch=3,cex=1,lwd=2,join=F,doses=F,legend=F,log=F, xlim=c(120, 144), layout=c(1, 1))



#19)function to calculate the mean and sd relative bias and relative RMSE and number of profils out of a p interval
setwd("../Runs")
PMload(1)
compauc<-function(start=x, end=y, prob=p, runnumber=z){
  post <- get(paste("post",runnumber,sep="."))
  AUC_3pts_lss<-makeAUC(post,pred~time,start =start,end=end, icen="median")
  names(AUC_3pts_lss)[2]<-"AUC_3pts_lss"
  head(AUC_3pts_lss)
  #auc trapezoid
  mdata_b<-subset(mdataval,evid==0)
  auc_trap<-makeAUC(mdata_b,out~time,start = start,end=end)
  names(auc_trap)[2]<-"auc_trap"
  head(auc_trap)
  #file with the 2 aucs
  comp_auc<-merge(auc_trap,AUC_3pts_lss )
  #relative bias
  comp_auc$bias<-(comp_auc$AUC_3pts_lss-comp_auc$auc_trap)/comp_auc$auc_trap
  #relative square bias
  comp_auc$sbias<-((comp_auc$AUC_3pts_lss-comp_auc$auc_trap)/comp_auc$auc_trap)*((comp_auc$AUC_3pts_lss-comp_auc$auc_trap)/comp_auc$auc_trap)
  delet<-comp_auc[comp_auc$AUC_3pts_lss=="Inf", ]
  comp_auc <- comp_auc[!comp_auc$id%in%delet,]
  #RMSE
  #relative root mean square error
  rmse<-sqrt(mean(comp_auc$sbias))
  #mean,sd, range, number of outside +/- p interval bias
  mean=mean(comp_auc$bias, na.rm=T)
  sd=sd(comp_auc$bias)
  range=range(comp_auc$bias)
  number_out=sum(sum(comp_auc[,4]>p)+sum(comp_auc[,4]<=-p))# replace p for limits of your auc exclusion ex 0.1 or 0.2
  return(list(mean_rel_bias=mean,sd_rel_bias=sd,range_rel_bias=range,number_out=number_out,rmse_rel_bias=rmse))
  
}
compauc(start=0,end=24,prob=0.2, runnumber=1)#




